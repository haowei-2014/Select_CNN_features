
%% bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference]{IEEEtran}

\usepackage{comment}
\usepackage{lineno,hyperref}

% Begin my packages
%\usepackage[labelformat=simple,font=small]{subcaption}
%\usepackage{subfigure}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup[sub]{font=scriptsize}
\usepackage{epstopdf}
\usepackage{lscape}
\usepackage{longtable}
\usepackage{url}
%\captionsetup{belowskip=5pt,aboveskip=4pt}   % control the space between caption and table
\usepackage{array}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algorithm}
%\usepackage{algcompatible}
%\usepackage[linesnumbered]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{multicol}
\usepackage{csquotes}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}
\newcommand{\lowerromannumeral}[1]{\romannumeral#1\relax}

\usepackage{amsmath}
\usepackage{comment}
%\renewcommand\thesubfigure{(\alph{subfigure})}
\usepackage{mfirstuc}
\usepackage[acronym]{glossaries}
\makeglossaries

% Acronym definitions
\newacronym{NB}{NB}{naive Bayes}


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf

\else

\fi


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%

\title{Experimental Results of Autoencoder Features With/Without Fine-tuning}


\begin{comment}

\author{\IEEEauthorblockN{Michael Shell}
\IEEEauthorblockA{School of Electrical and\\Computer Engineering\\
Georgia Institute of Technology\\
Atlanta, Georgia 30332--0250\\
Email: http://www.michaelshell.org/contact.html}
\and
\IEEEauthorblockN{Homer Simpson}
\IEEEauthorblockA{Twentieth Century Fox\\
Springfield, USA\\
Email: homer@thesimpsons.com}
\and
\IEEEauthorblockN{James Kirk\\ and Montgomery Scott}
\IEEEauthorblockA{Starfleet Academy\\
San Francisco, California 96678--2391\\
Telephone: (800) 555--1212\\
Fax: (888) 555--1212}}

\end{comment}


% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}

\end{abstract}

% no keywords



\IEEEpeerreviewmaketitle



\section{Introduction}



In the previous experiments (ICDAR, HIP, and DAS), Kai, Oussama, and Me used Mathias' framework to extract autoencoder features.
Generally, these features were learned by autoencoders without fine-tuning (if I am correct).
In our HIP paper and also my thesis, we observed that: \textit{a significant number of autoencoder features are redundant or irrelevant for layout classification.}
This observation can be explained by the fact that autoencoders (without fine-tuning) are trained for reconstruction, rather than classification.
%Thus some of the learned features may be not helpful for layout classification.

Autoencoders with fine-tuning are a kind of supervised feature learning technique (if I am correct).
The learned features are supposed to be optimal or nearly optimal for classification.
In other words, the learned features are supposed to be less redundant and less irrelevant for classification, compared to features learned by autoencoders without fine-tuning.
This is an important motivation of our following experiments.


In the initial experiments, we investigate redundancy and irrelevance of features leaned by autoencoders with/without fine-tuning.
We use autoencoder features to perform layout analysis, which classifies pixels on an image into five classes: out of page, background, comments, decorations, and text.
Sequential Forward Selection (SFS) is used for feature selection.
The results are shown in Table~\ref{table:AE}.

Table~\ref{table:AE} shows that for all classifiers using features learned by autoencoders with/without fine-tuning, generally feature dimension is decreased significantly, and comparable accuracy is obtained.
So from Table~\ref{table:AE}, we have two observations.
(1) Regarding features learned with fine-tuning, a significant number of them are redundant or irrelevant for layout classification.
This observation is similar with our previous observations about features learned by autoencoders without fine-tuning.
(2) Regarding redundancy and irrelevance, there is no significant difference between features learned with and without fine-tuning.
In other words, regardless of whether autoencoder features are learned with or without fine-tuning, they are equally redundant and irrelevant for layout classification in general.







\section{CNN Architecture}


\section{Experimental Results}


\begin{table*}[t]
\begin{center}
\caption{Investigation of redundancy and irrelevance of features leaned by autoencoders with fine-tuning.
The used classifiers are Support Vector Machine (SVM), Naive Bayes (NB), a decision algorithm (C4.5), and k-Nearest Neighbors (k-NN) where k=1.
} 
\label{table:AE}
\begin{tabular}{C{3cm} | C{3cm} | C{3cm} | C{3cm}} 
\toprule 
Classifier & Features & Dimension & Accuracy \\
\midrule
\multirow{2}{*}{SVM}  & Full features & 46 &  \\

 &  Selected features &  &  \\

\cmidrule(r){1-4}

\multirow{2}{*}{NB}  & Full features & 46 & 88.10 \\

 &  Selected features & 7 & 85.81 \\

\cmidrule(r){1-4}

\multirow{2}{*}{C4.5}  & Full features & 46 & 77.34 \\

 &  Selected features & 9 & 81.83 \\

\cmidrule(r){1-4}

\multirow{2}{*}{k-NN}  & Full features & 46 & 77.91 \\

 &  Selected features & 22 & 77.28 \\


\bottomrule
\end{tabular}
\end{center}
\end{table*}


\section{Conclusions}


\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}


\end{thebibliography}




% that's all folks
\end{document}


